---
title:  "프로젝트 계획, Crawling 권한 확인 robots.txt"
excerpt: "크롤링에 앞서 robots.txt파일을 봐야하는 이유를 설명한다."
toc: true
toc_sticky: true # 화면 넘어갈때 고정 여부
# toc_label: "페이지 주요 목차" 를 직접 입력할 수 있다.
categories:
  - Project01
tag:
  - Crawling
  - 프로젝트 계획
last_modified_at: 2020-07-30T09:00:00-11:00
---

# 1. 프로젝트 계획

## 1) 무엇을(What-) ?

지금부터 시작할 프로젝트는 해외축구 공신력 사이트의 정보를 제공하고,<br>
보다 클린한 커뮤니티 역할을 수행하는 홈페이지를 만들 것이다.

## 2) 어떻게(how-) ?

주요 기능은 크롤링(Crawling)이 될 것. 

## 3) 왜(Why-) ?

블로그 작성자는 축구팬으로서 다음과 같은 기능을 제공하는 커뮤니티 사이트를 만들고자 한다.<br>

① 국내 축구팬들에게 보다 객관적인 정보를 제공하고자 함.<br>
② 국내 커뮤니티에는 특정 팀을 조롱하거나 정치, 연예관련 언급을 금하는 시스템을 구축하여 보다 클린한 커뮤니티 활성화.<br>
③ 이적관련 해외 공신력을 추정하는 프로그램을 내부로 구축하여 사람들에게 제공한다.<br>
④ 여건이 된다면 K1~K4까지 영역을 확대코자 함.

---

# 2. 언어(Language)

- [Server] AWS - Ubuntu20.08
- [Frontend] Vue.js
- [Backend] Java(Spring-Framework)
- [DataBase] Mysql

---

# 3. 크롤링과 robots.txt

## 1) 크롤링(Crawling)이란?<br>

크롤링(crawling) 혹은 스크레이핑(scraping)은 웹 페이지를 그대로 가져와서 거기서 데이터를 추출해 내는 행위다.

## 2) robots.txt

우선 크롤링에 앞서 robots.txt의 역할을 알아보자.
모든 사이트(?)에는 robots.txt가 있다.<br>
가령, [https://fifabell.github.io/robots.txt]<br 
위 사이트처럼 RootUrl/robots.txt를 작성해서<br>
정보 제공자가 이를 활용해 여기는 어떤 사이트고, <br>
어떤 정보가 제공되는 사이트인지, 사이트의 어느 부분을 스크랩하면 안되는 지 알려준다.

## 3) Robots.txt 기본 문법

- User-Agent: 웹사이트 관리자가 어떤 종류의 로봇이 크롤링을 하는지 알 수 있게 돕는다.

- `Disallow`: 이 명령은 어떤 웹페이지 URL을 크롤링 하지 않아야 하는지 알려줍니다.

- Allow: 모든 검색엔진이 이 명령을 인식하지는 않지만 특정 웹페이지나 디렉토리에 접근하라는 명령입니다.

- Crawl-delay: 검색엔진 스파이더 봇이 서버를 과도하게 사용하지 못하도록 대기하라는 명령입니다.

- Robots Exclusion Protocol

- Sitemaps Protocol

---


# # 참조 사이트

- [robots.txt 참조 블로그](https://limelightkr.co.kr/robots-txt-그게-뭐죠/)